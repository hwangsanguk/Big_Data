# 20200716

### Part 3. 파이썬을 활용한 웹 크롤링 & 자연어 데이터 분석

mac은 mac에 맞게 서치하여 다운로드 하여야함

--------------

~~~~
-Web Scraping with Beatuiful Soup
-Basic texst data analysis
-WorldCloud for text data
-Advanced web scraping with Selenium
-(+Browserless web scraping with PhantomJs)
~~~~

**Web crawling?**

- bot 이 web link를 통해 돌아다니는 것
  => www의 링크들을 누르고 다님, 그 링크들을 검색 DB에 저장함

- Web scraping!

  - webpage에서 원하는 자료를 **긁어** 오는 것

  

### Web scraping

- 코딩 없이 ScrapeStorm 을 통해 scraping할 수 있음

- ~~~html
  <html>
  
  <head>
  <script></script>
  </head>
  
  <body>
  <h1~6></h1~6>
  <p>이것은 
      <span style='color: red'>
          메인
      </span> 
      문단
  </p>
     <img src='' />
  <input></input>
      <p>이것은 
      <span style='color: red'>
          메인
      </span> 
      문단
  </p>
     <img src='' />
  <input></input>
  <p>이것은 
      <span style='color: red'>
          메인
      </span> 
      문단
  </p>
     <img src='' />
  <input></input>
  <a href='http://www.naver.com'>네이버</a>
   tag name/ tag attribute(태그 속성)   
  <button></button>
  <div></div>
  
  
  <table>
  	<tr>
  		<th>
  		</th>
  		<td>
  		</td>
  	</tr>
  </table>
  <li></li>
  
  </body>
  
  </html>
  ~~~

- 태그 이름을 기준으로 꺼낼것이기 때문에 알고 있으면 대응하기가 편함

- ~~~~
  The process of web scraping(simple)
  1. Webpage 의 전체 HTML code를 가져옴
  2. 크롤링을 원하는 부분의 HTML태그를 찾는다. (class & ID)
  3. 해당 태그를 꺼낸 다음 태그를 제거하고 내부의 데이터만 뽑아낸다.
  ~~~~

- ~~~
  The process of web scraping (detail)
  ~~~



#### Web scraping Basic

~~~~python
!pip install beautifulsoup4==4.7.1

from bs4 import BeautifulSoup # urlopen의 html코드를 원하는것을 쓸 수 있도록 도와줌
from urllib.request import urlopen # urlopen은 url 값을 기준으로 html 코드를 가져옴
~~~~

~~~python
# 검색하고 싶은 단어 입력하기
word = 'happiness'

# 불러오려는 url 입력하기 
# 디폴트 url에 string 타입의 word 변수를 합쳐서 url 변수 생성
url = 'https://alldic.daum.net/search.do?q=' + word

# urlopen 함수를 통해 web 변수를 생성
web = urlopen(url)   # urlopen(url).read().decode('utf-8')

# BeautifulSoup으로 web 페이지상의 HTML 구조를 파싱
web_page = BeautifulSoup(web, 'html.parser')

print(web_page)
~~~

~~~python
# 찾는 단어 (대상이 하나일 때)


#find('태그이름',{attribute key : attribute value})

box1 = web_page.find('span', {'class': 'txt_emph1'}) # 찾다

print(box1)
print(box1.get_text()) # 태그를 걷어내고 내부의 텍스트만 꺼내고 싶을 때 


# 단어의 뜻 (대상이 여러개일 때)

box2 = web_page.find_all('span', {'class': 'txt_search'}) # 모두 찾다
for tag in box2:
    print(tag.get_text())
    
    
# 단어와 단어의 뜻 출력하기

print(box1.get_text()) # get_ + Tab!
print()

for definition in web_page.find_all('span', {'class': 'txt_search'}):
    print(definition.get_text().strip()) # 앞부분의 공백을 지워주기 위해 strip() 적용
~~~

HTTP: hyper text transfrom protocol

client ==terminal(web brower)  <-> server



www.naver.com => 네이버 서버 컴퓨터 에게 데이터 요청을 함

HTTP Request (POST=create / GET=read/ PUT=update /DELETE=delete 등의 형태로 보냄)

=> HTTP Response :  html / XML / JSON / byte ~



#### 영화 제목 출력하기

~~~python
from bs4 import BeautifulSoup 
from urllib.request import urlopen


# 불러오려는 url 입력하기 (IMDb - Guardians of the Galaxy Vol. 2 (2017) )
url = 'http://www.imdb.com/title/tt3896198/?ref_=nv_sr_6'

# urlopen 함수를 통해 web 변수를 생성
web = urlopen(url)

# BeautifulSoup으로 web 페이지상의 HTML 구조를 파싱
web_page = BeautifulSoup(web, 'html.parser')


# 영화 제목을 출력한다

title = web_page.find('h1')

print('Movie Title:')
print(title.get_text())


# 영화 요약을 출력한다

summary = web_page.find('div', {'class': 'summary_text'})

print('Movie Summary:')
print(summary.get_text().strip())


# 감독 이름을 출력한다 (바깥의 box를 먼저 catch 후, 그 안의 tag를 catch!)
# find().find() 사용이 가능하다!
director = web_page.find('div', {'class': 'credit_summary_item'}).find('a')

print('Director:')
print(director.get_text().strip())
~~~



#### Brunch 기사글 출력하고 저장하기

~~~~python
# Save multiple articles automatically

# @@@@ 님의 글 10개를 자동으로 가져와 저장하는 코드를 작성해보세요!
# Hint 1 : str에는 str만 더할 수 있어요!
# Hint 2 : @@@@님 글의 URL은 몇 번부터 시작할까요?

for i in range(10):
    
    try:
        url = 'https://brunch.co.kr/@@@@/' + str(i)
        web = urlopen(url)
        source = BeautifulSoup(web, 'html.parser')

        with open('brunch_all.txt', 'a', encoding = 'utf-8') as f:

            all_text = source.find('div',{'class': 'wrap_body'})
            article = all_text.find_all('p')

            for content in article:
                print(content.get_text())
                f.write(content.get_text() + '\n')
    except:
        pass
    
    
# (Additional) 예외처리 (try & except pass 적용해보기)

# try, except 는 반드시 한 세트로 해야함 , 그 뒤에 2개의 더 예외 처리가 있음
~~~~

조심 : 서버적인 문제 => 많은 것을 하는 순간 500error 가 나올 수 있음  => 돌릴때 마다 쉬는 시간을 줘 어느 정도 방지, cloud 에서 돌릴 수도 있음

법적인 측면 : 연구용으로 쓰고 폐기 해야함 

robots.txt => 어떤 페이지에 대해서 허용 하는지 비허용 하는지에 대해서 알려주는 txt



### 데이터 전처리(analysis for text data)

토큰화(Tokenization) : 코퍼스(corpus)에서 토큰(token)이라 불리는 단위로 나누는 작업을 토큰화라고 한다
우리가 받아온 텍스트를 바로 사용할수 없어서 이 토큰화를 한 뒤 단계를 거쳐 사용하게 된다.

텍스트 데이터를 str 자료형으로 준비
=> Tokenize(형태소 분석), POS Tagging(Part-of-speech, 품사 표시), Stopwords 제거(불용어 제거)
=> 단어 갯수 카운팅 & 단어 사전 생성, 단어 사전 기반 데이터 시각화(+ 머신러닝/ 딥러닝 모델 적용)



**nltk** library(Natural Language Toolkit)를 이용하여 Text Processing을 위한 전처리를 실습한다.

1. 영어 문장 토큰화 하기

   ~~~~~python
   # Test processing을 위해 nltk package 를 import!
   # !pip install nltk==3.4
   
   import nltk
   ~~~~~

   ~~~~
    nltk.download()
   *******중요*********
      단어들의 감정 분류 => Corpora
      자연어를 대상으로한 모델 => model
   
   아래 명령어를 통해 download 대화상자를 열어 패키지를 다운로드 받아야 합니다.
   인터넷 속도 저하 시 매우 오래 걸리므로, 패키지 설치 경로만 확인한 다음 [ (nltk, downloaded) nltk_data.zip ] 의 파일들을 복사합니다.
   
   경로 예시 : "C:\Users{컴퓨터 이름}\AppData\Roaming\nltk_data"
   
   nltk_data 폴더 안에 corpora, taggers, tokenizers 폴더가 바로 위치하도록 복사해줘야 합니다.
   ~~~~

   ~~~~~~~python
   # 전처리하고자 하는 문장을 String 변수로 저장한다
   sentence = 'NLTK is a leading platform for building Python programs to work with human language data. It provides easy-to-use interfaces to over 50 corpora and lexical resources such as WordNet, along with a suite of text processing libraries for classification, tokenization, stemming, tagging, parsing, and semantic reasoning, wrappers for industrial-strength NLP libraries, and an active discussion forum.'
   
   # 각 문장을 토큰화한 결과를 출력한다
   nltk.word_tokenize(sentence)  # 문장을 '단어 수준에서' 토큰화해 출력한다 
   ~~~~~~~

2. 영어 문장 품사 태깅(POS tagging) 하기

   ~~~~~python
   # 각 문장을 토큰화한 후 품사를 태깅하여 결과를 출력한다
   
   tokens = nltk.word_tokenize(sentence)  # 문장을 토큰화한다
   nltk.pos_tag(tokens)  # 토큰화한 문장을 대상으로 품사를 태깅("POS" Tagging)하여 출력한다
   # tuple 이 되므로 index기반으로 꺼낼 수 있다.
   ~~~~~

3. Stopwords (불용어) 제거하기
   불용어는 제외 하는 경우도 있고, 불용어에 추가하는 경우도 있다.

   ~~~~python
   # nltk 모듈에서 Stopwords를 직접 불러온다
   from nltk.corpus import stopwords
   
   
   # 영어의 stopwords를 불러와 변수에 저장한다 (stopwords에 속하는 "단어" 리스트)
   stopWords = stopwords.words('english') # 지원 언어 목록 : stopwords.fileids()
   
   # How many stop words
   print(len(stopWords))
   print()
   
   # stop words 출력
   print(stopWords)
   
   
   # 문장에서 stopwords 제거
   
   result = []  
   # stopwords가 제거된 결과를 담기 위한 리스트를 생성한다
   
   for token in tokens:  
       # for문을 통해 각각의 token이 stopwords인지 아닌지를 판별해 결과에 저장한다
       if token.lower() not in stopWords:  
       # 만약 소문자로 변환한 token이 stopWords 내에 없으면:
           result.appen(token)  
           # token을 리스트에 더해준다
   
   print(result)  # 결과를 출력한다
   
   
   # stopwords에 쉼표(,)와 마침표(.) 추가하여 다시 적용하기
   
   stop_words = stopwords.words("english")
   # stop_words == list
   stop_words.append(',')
   stop_words.append('.')
   
   result = []
   # stopwords가 제거된 결과를 담기 위한 리스트를 생성한다
   
   for token in tokens:
   # for문을 통해 각각의 token이 stopwords인지 아닌지를 판별해 결과에 저장한다
       if token.lower() not in stop_words:  
       # 만약 소문자로 변환한 token이 stopWords 내에 없으면:
           result.append(token)  
           # token을 리스트에 첨부한다
   
   print(result)  # 결과를 출력한다
   ~~~~

4.  영화 리뷰 데이터 전처리하기 -

   ~~~~python
   # WordNetLemmatizer 객체 생성
   lemmatizer = nltk.wordnet.WordNetLemmatizer()
   
   
   # WordNetLemmatize는 더 정확한 분석을 위해 PoS 정보를 추가로 입력받을 수 있음 (n : 명사 v : 동사 a : 형용사 r : 부사)
   # default == n(명사) 이므로 'cats', 'geese' 들은 기본명사형인 'cat','geese'로 결과가 출력됨
   # 'ran'은 동사를 나타내는 PoS 정보인 'v'를 함께 입력해주어야 제대로 결과를 확인할 수 있음
   # 'better'도 마찬가지로, '형용사(a)'라는 정보를 함께 입력해주어야 원형인 'good'을 제대로 출력해줌
   
   print(lemmatizer.lemmatize("cats")) 
   # lemmatize한 결과를 출력한다
   print(lemmatizer.lemmatize("geese"))
   ~~~~

   ~~~~~python
   # Stopwords
   stop_words = stopwords.words("english")
   stop_words.append(',')
   stop_words.append('.')
   
   with open('moviereview.txt', 'r', encoding='utf-8') as file:  # 읽기 형식('r')로 지정하고 인코딩은 'utf-8'로 설정한다
       lines = file.readlines()  # readlines 함수로 텍스트 파일의 내용을 읽어 "리스트로 저장"한다
   
   sentence = lines[1] 
   tokens = nltk.word_tokenize(sentence)  
   
   # for문을 통해 stopwords 제거와 lemmatization을 수행한다
   lemmas = []  # lemmatize한 결과를 담기 위한 리스트를 생성한다
   for token in tokens:  
       if token.lower() not in stop_words:  # 소문자로 변환한 token이 stopwords에 없으면:
           lemmas.append(lemmatizer.lemmatize(token))  # lemmatize한 결과를 리스트에 첨부한다
   
   print('Lemmas of : ' + sentence)  # lemmatize한 결과를 출력한다
   print(lemmas)
   ~~~~~

### 가장 많이 등장하는 단어 추출

1. 영화 dark knight 리뷰에서 많이 등장하는 명자 추출

   ~~~~~~~~python
   import nltk
   from nltk.corpus import stopwords
   from collections import Counter
   
   # 1) Stopwords 준비하기
   stop_words = stopwords.words("english")
   stop_words.append(',')
   stop_words.append('.')
   stop_words.append('’')
   stop_words.append('”')
   stop_words.append('—')
   #이후 정규식을 통해서 바꿔 줄 수 있음
   
   # 2) Text data 준비하기
   with open('darkknight.txt', 'r', encoding="utf-8") as f:  # 읽기 형식('r')로 지정하고 인코딩은 'utf-8'로 설정한다
       lines = f.readlines()  # readlines 함수로 텍스트 파일의 내용을 읽어 리스트로 저장한다
       
       
   # 3) Tokenizing
   tokens = []
   for line in lines:  
       # for문을 통해 각 줄에 접근한다
       
       tokenized = nltk.word_tokenize(line)  
       # 각 줄을 tokenize한다 (word_ + Tab!) /형태소들의 리스트이다
       for token in tokenized:
           if token.lower() not in stop_words:
               tokens.append(token)
               
   # 4) POS Tagging - 명사 종류만 모으기
   
   tags = nltk.pos_tag(tokens)  # tokenize한 결과를 품사 태깅한다
   
   wordList = []
   for word, tag in tags:  
       # for 문을 통해 각각의 (단어, 태그) 쌍에 접근
       
       if tag in ['NN', 'NNS', 'NNP', 'NNPS']:  
       # 만약 태그가 명사 종류이면:
       
           wordList.append(word.lower())  
           # 소문자로 변환한 후 리스트에 첨부한다
           
           
   counts = Counter(wordList)  
   # list 의 item 들의 출현 횟수를 알 수 있는 것
   #각 명사의 숫자를 센 결과를 변수에 저장한다 (Counter -> {'home': 15, '단어':출현횟수, ...})
   
   ~~~~~~~~

2. **토큰 개수 확인하기**

   ~~~~python
   stop_words = stopwords.words("english")
   stop_words.append(',')
   stop_words.append('.')
   stop_words.append('’')
   stop_words.append('”')
   stop_words.append('—')
   stop_words.append('else')
   stop_words.append('$')
   
   with open('darkknight.txt', 'r', encoding="utf-8") as f:  # 읽기 형식('r')로 지정하고 인코딩은 'utf-8'로 설정한다
       lines = f.readlines()  # readlines 함수로 텍스트 파일의 내용을 읽어 리스트로 저장한다
   
   tokens = []
   for line in lines:  # for문을 통해 각 줄에 접근한다
       tokenized = nltk.word_tokenize(line)  # 각 줄을 tokenize한다 => 각각 list 의  item 들로 저장이 됨
       for token in tokenized:
           if token.lower() not in stop_words:
               tokens.append(token)
               
   corpus = nltk.Text(tokens) 
   print(corpus.tokens)
   ~~~~

   

3. 토큰의 등장 횟수 시각화 하기

   ~~~python
   import matplotlib.pyplot as plt
   import re # 정규표현식 == Regular Expression
   
   sample =re.match('^[a-zA-Z+]','korean')
   
   import matplotlib.pyplot as plt
   import re # 정규표현식 == Regular Expression
   
   stop_words = stopwords.words("english")
   stop_words.append('else')
   
   with open('darkknight.txt', 'r', encoding="utf-8") as f:  # 읽기 형식('r')로 지정하고 인코딩은 'utf-8'로 설정한다
       lines = f.readlines()  # readlines 함수로 텍스트 파일의 내용을 읽어 리스트로 저장한다
       
   tokens = []
   for line in lines:  # for문을 통해 각 줄에 접근한다
       tokenized = nltk.word_tokenize(line)  # 각 줄을 tokenize한다
       for token in tokenized: # 각 Token이
           if token.lower() not in stop_words: # Stopwords 리스트에 포함되어 있지 않으며,
               if re.match('^[a-zA-Z]+', token): # 특수 기호에 해당하지 않을 경우,, 영어 글자로 시작하지 않으면 false
                   tokens.append(token) # Token list 에 추가해준다.
                   
   corpus = nltk.Text(tokens) 
   
   plt.figure(figsize=(10, 3)) 
   plt.title('Top 25 Words',fontsize=30)
   
   corpus.plot(25) # 25개까지만 확인, "plot" the data
   ~~~

4. 문맥 상 유사한 단어 출력

   ~~~~python
   with open('darkknight.txt', 'r', encoding="utf-8") as f:  # 읽기 형식('r')로 지정하고 인코딩은 'utf-8'로 설정한다
       lines = f.readlines()  # readlines 함수로 텍스트 파일의 내용을 읽어 리스트로 저장한다
   
   tokens = []
   for line in lines:  # for문을 통해 각 줄에 접근한다
       tokenized = nltk.word_tokenize(line)  # 각 줄을 tokenize한다
       for token in tokenized: # 각 Token이
           if token.lower() not in stop_words: # Stopwords 리스트에 포함되어 있지 않으며,
               if re.match('^[a-zA-Z]+', token): # 특수 기호에 해당하지 않을 경우,
                   tokens.append(token) # Token list 에 추가해준다.
   
   corpus = nltk.Text(tokens)
   
   print('Similar words : ')
   corpus.similar('batman') # 유사한 단어들 (similar words)
   ~~~~

   

5. 텍스트의 연어(collocation) 출력 하기

   ~~~~python
   with open('darkknight.txt', 'r', encoding="utf-8") as f:  # 읽기 형식('r')로 지정하고 인코딩은 'utf-8'로 설정한다
       lines = f.readlines()  # readlines 함수로 텍스트 파일의 내용을 읽어 리스트로 저장한다
       f.close()  # 파일을 닫는다
       
   tokens = []
   for line in lines:  # for문을 통해 각 줄에 접근한다
       tokenized = nltk.word_tokenize(line)  # 각 줄을 tokenize한다
       for token in tokenized: # 각 Token이
           if token.lower() not in stop_words: # Stopwords 리스트에 포함되어 있지 않으며,
               if re.match('^[a-zA-Z]+', token): # 특수 기호에 해당하지 않을 경우,
                   tokens.append(token) # Token list 에 추가해준다.
           
   corpus = nltk.Text(tokens) 
   print('Collocation')
   corpus.collocations() # co-location
   ~~~~

   

### Text Analysis 실습

**TF-IDF(Term Frequency - Inverse Document Frequency)**

tf(d,t) : 단어의 빈도수 
idf(d,t) : inverse document frequency
nd: 전체 문서의 수 
df(t) : 단어 t를 가진 문서의 수
**idf(d,t) = log(nd/ (1+df(t)) )**

**tf-idf(d,t) = tf(d,t)*idf(d,t)**

#### 코사인 유사도

두 벡터의 유사한 정도를 의미 

- **sklearn을 통한**  코사인 유사도 검증

  1. import

  ~~~~python
  # 유사도 분석에 필요한 패키지를 불러온다
  from sklearn.feature_extraction.text import TfidfVectorizer
  from sklearn.metrics.pairwise import cosine_similarity
  ~~~~

  2. 비교할 2개의 txt 파일을 가져와 그것을 쪼갠 뒤 다시 저장

  ~~~~python
  with open('shawshank.txt', 'r', encoding = 'utf-8') as f:  
      doc1 = ''  # 리뷰 데이터를 담기 위한 String 변수 생성
      lines = f.readlines()  # 영화 리뷰 파일의 모든 라인을 읽어와 리스트로 저장
      for line in lines:  # for문을 통해 lines에 있는 모든 텍스트를 doc1에 이어 붙임
          doc1 += line
  
  #     lines = f.readlines()
  #     docl = ''.join(lines)
          
  with open('godfather.txt', 'r', encoding= 'utf-8') as f: 
      doc2 = ''  # 리뷰 데이터를 담기 위한 String 변수 생성
      lines = f.readlines()  # 영화 리뷰 파일의 모든 라인을 읽어와 리스트로 저장
      for line in lines:  # for문을 통해 lines에 있는 모든 텍스트를 doc2에 이어 붙임
          doc2 += line
  ~~~~

  3. 두개를 하나의 list로 합치기

  ~~~~python
  corpus = [doc1, doc2]  # doc1, doc2를 합쳐 corpus list를 생성
  #corpus[0]
  ~~~~

  4. TfidfVectorize() 객체 변수 생성

  ~~~~python
  vectorizer = TfidfVectorizer()  # TfidfVectorizer() 객체 변수 생성
  ~~~~

  5.  4번의 값 vector 화 하고 그것을  조밀하게 만들기

  ~~~python
  # fit_transform()를 통해 corpus의 텍스트 데이터를 벡터화해 X에 저장하고 X를 dense한 matrix로 변환
  # raw counts have been normalized against document length, terms that are found across many docs are weighted down.
  # 모델을 우리에게 맞게 학습시키기
  X = vectorizer.fit_transform(corpus).todense()
  ~~~

  6. 5번 matrix 확인 및 m by n matrix인지 확인

  ~~~~python
  print(type(X))
  print(X.shape) # 모양 확인 : 2행 3276열의 행렬 (각 행은 각 영화 리뷰 데이터의 word(열) 출현 빈도에 대한 dense matrix)
  ~~~~

  7. sklearn의 cosine_similarity를 사용하요 코사인 유사도 검증

  ~~~~python
  print("Similarity between 'The Shawshank Redemption' and 'The Godfather': ", cosine_similarity(X[0], X[1])) # 코사인 유사도
  ~~~~

  그 결과 [0.9437827] 정도로 나왔고 1과  가깝게 나왔기 때문에 The Godfather 과 The Shawshank Redemption의 리뷰가 비슷하다고 할 수 있음

