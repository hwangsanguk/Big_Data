# 20200715

## Part 2. 파이썬 정형 데이터 분석 & 데이터 시각화

데이터를 수집하는 부분 
과거 에는 손으로 수기 작성 후 컴퓨터에 옮김
이후 CRUD 데이터를 모아서 저장 &  Sensor Data



**Various data type**

정형 데이터 : 깔끔하게 떨어지는 액셀 데이터 (연산의 적용이 가능)

반 정형 데이터 : HTML, 시스템 log 등

비정형 데이터 :  이미지, 영상, 음성, 문서 등 (비정형 데이터를 정형 데이터로 바꾸는게 ML 과 DL의 관점) 음성 영역이 가장 어렵다



Various data collection -Owned data

- google analytics (도구의 관점)
- Elastic Stack or ELK Stack (개발자 관점에서 좋음)
- Zeppelin (서버에서 구동하는 협업 도구)



Various data collection - Unokwned data

- APIs (신분을 통한 서비스 제공) , 공익적인 목적
- Bots(Web crawler, Web scraper) 



Various data collection - Public data & Open data(APIs & files)

- 공공 데이터 포털 
- 국가 통계 포털
- MDIS

데이터



#### Pandas

- import 하기

~~~~python
import numpy as np
import pandas as pd
import seaborn as sns
# import matplotlib.pyplot as plt
~~~~

- 엑셀 파일 읽기

  ~~~python
  # 엑셀 파일 읽기 df= DataFrame
  pd.read_excel(파일명,encoding='utf-8')
  df = pd.read_excel('animals.xlsx', encoding='utf-8')
  ~~~

- DataFrame에서 0에서 1씩 증가하는 index 가 있음

- DataFrame / Series (연속적인) => 아래로 연속적인(열이 연속)

- ~~~~
  df.head() # 머리! 가장 위의 5개 행이 나오는데 5개가 default
  
  df.tail() # 꼬리! 가장 아래의 5개 행이 나오는데 5개가 default
  
  df.describe()  # 묘사하다.
  갯수, 평균, 표준편차 등이 나옴
  
  df.info() # 정보
  열마다의 정보가 추가적으로 나옴
  
  # null data == 빠진 데이터
  # =>결측치 처리 (Missing data handling)
  
  # => 랜덤 값으로 채우기
  # => 평균 값/중간 값 으로 채우기 mean/median 값으로 채우기
  # => 구간별로 나눠서 구간의 대표값(mean/median) 으로 : 직원들 데이터 -< 직급 열의 구간
  # => ML/DL 모델을 활용해서 예측해서 넣기 : age열의 결측치 존재 => age열 제외 나머지 열로 예측
  ~~~~

- **DataFrame에서 [행] 꺼내기**

  ~~~~python
  df.loc[3] # "위치"를 영어로? & 대괄호에 유의 (함수의 소괄호가 아님) 3이 뜻하는 것은 이름 옆에 새로 생긴 열의 숫자를 뜻함
  
  type(df.loc[3]) # 어떤 자료형(data type)일까요?
  result = pandas.core.series.Series
  
  df.loc[[3]] # Series가 아닌 DataFrame으로 꺼내려면 [[번호]]를 통해서 꺼내야함
  
  df.loc[[3, 6, 8]] # 여러 개의 행을 꺼내기 (Key들을 list로 줘야 합니다.)[[a, b, c]] 의 형태로 꺼내야함
  
  df.loc[3:6, 'name':'feathers'] # 범위로 지정해 꺼내기 & 열까지 범위 지정하기 (뒤까지 모두 포함하는 범위임을 유의, index가 아니라 name)
  
  
  df["name"].str.contains("ar") # 문자열.포함하다 (선택한 열의 문자열에 특정 문자열이 포함되어있는지를 체크)
  
  
  sum(df["name"].str.contains("ar"))
  
  df.loc[행,열]
  
  df.loc[ar이 들어있는 행만 , 모든열]
  df.loc[df["name"].str.contains("ar"), :] # 선택한 열의 문자열에 특정 문자열이 포함된 열들만 선택하고자 할 경우
  
  index기반 
  df.iloc[3:6, 0:3] #열을 name이 아니라 번호로 지정하고 싶어서 iloc를 사용함
  ~~~~

- **DataFrame에서 [열] 꺼내기**

  ~~~python
  df['name']
  
  df['name'].head() # 하나의 열 출력하기
  
  df[['name','hair','feathers']] # 여러개의 열을 출력할 때 [[a,b,c,]]의 list형태로 
  ~~~

- DataFrame 다루기 with Advanced function

  ~~~python
  # 기존 df 에서 일부 열만 떼어서 새로운 df 만들기
  df_new = df[['name', 'hair', 'feathers', 'eggs', 'milk', 'type']] # list형태로 열을 만들어 줘야함
  
  df_new.head() # df_new의 상위 5개를 출력해라
  
  
  # hair 열에 1을 더한 새로운 열을 new_hair 로 추가하기
  
  df_new['new_hair'] = df_new['hair'].apply(lambda x : x+1) # 이 결과 type 뒤에 new_hair가 존재하고 값드링 1씩 증가함
  아래에 더 자세하게 설명
  ~~~

  ~~~~python
  새로운 데이터 추가하기
  df_new['new_hair'] = [~~~~] # 리스트로 만들어서 넣어줌, 리스트의 데이터 갯수(len) == DataFrame의 열의 길이 => 단순 반복작업이 심함
  ~~~~

  ~~~~python
  # hair 열에 1을 더한 새로운 열을 new_hair 로 추가하기
  
  df_new = df[['name', 'hair', 'feathers', 'eggs', 'milk', 'type']]
  
  df_new['new_hair'] = df_new['hair'].apply(lambda x : x+1)
  
  
  # Type 을 index 로 하는 새로운 df 만들기 (엑셀의 pivot table 과 유빌ㄷ사)
  
  # pd(판다스).pivot_table(피벗테이블)( DataFrame, index='기준 정하기', aggfunc=np.sum)
  pivot_df = pd.pivot_table(df_new, index='type', aggfunc=np.sum) # Shift+Tab!
  
  # Aggregation function == 집계 함수
  
  
  # 열 삭제하기
  del pivot_df['new_hair'] 
  
  # 행 삭제하기
  pivot_df = pivot_df.drop([3]) #Database 에서 data point 를 drop! 즉, index에서 숫자 3인 행을 삭제
  
  # 열 이름 바꾸기 
  # 원하는 열만 이름을 바꾸기
  # '이름을 바꿔주다'?
  # new_pivot = pivot_df.rename( columns = {'eggs':'산란', 'feathers':'깃털'}) => 원본을 지키는 방법
  
  pivot_df.rename(columns = {'eggs':'산란', 'feathers':'깃털'}, inplace=True) # inplace 옵션 == 덮어쓰기 를 통해 덮어쓸 수 있음
  
  
  # 산란 수 기준으로 정렬하기
  
  pivot_df.sort_values(by='산란', inplace=True) # 내용(value)을 기준으로 정렬(sort), inplace=True : 덮어쓰기
  pivot_df.head()
  
  
  # 산란 수 기준으로 내림차순 정렬하기
  # ascending 을 통해서 내림 차순 가능
  pivot_df.sort_values(by='산란', ascending= False, inplace=True) 
  pivot_df.head()
  
  # 얕은 복사와 깊은 복사
  
  pivot_df_2 = pivot_df # shallow copy
  pivot_df_3 = pivot_df.copy() # deep=True, deep copy
  
  pivot_df_2.head(3) # 원본 DataFrame인 pivot_df와 연결되어 있습니다.
  
  pivot_df_3.head(3) # 원본 DataFrame인 pivot_df로부터 분리되어 별도로 존재합니다.
  ~~~~

  



## 서울시 범죄현황 통계자료 분석 및 시각화

1. **데이터 입력 및 데이터 전처리**

   ~~~~~python
   - 경찰서를 구별로 정리
   - 범죄별로 검거율 계산
   - 인구 데이터 merge 하기 (인구 data는 밖에서 가져와야함)
   ~~~~~

   ~~~~~python
   import numpy as np
   import pandas as pd
   import seaborn as sns
   
   import matplotlib.pyplot as plt
   from matplotlib import font_manager, rc 
   
   ~~~~~

   **데이터 전처리**

   ~~~~python
   # 서울시 관서별 5대 범죄 발생 & 검거 현황 @ data.go.kr
   # 원본 데이터 및 전처리 작업을 위한 파이썬 코드 @ folder named [ Original data source & data preprocessing (5대범죄 & 인구수) ] 
   
   df = pd.read_excel('관서별 5대범죄 발생 및 검거.xlsx', encoding='utf-8') # 엑셀 파일 읽기?
   
   df.head() # 데이터의 윗 부분만 살펴보려면?
   ~~~~

   **경찰서를 구별로 정리**

   ~~~~python
   # 서울시 경찰청의 소속 구 @ https://goo.gl/MQSqXX
   police_to_gu = {'서대문서': '서대문구', '수서서': '강남구', '강서서': '강서구', '서초서': '서초구',
                   '서부서': '은평구', '중부서': '중구', '종로서': '종로구', '남대문서': '중구',
                   '혜화서': '종로구', '용산서': '용산구', '성북서': '성북구', '동대문서': '동대문구',
                   '마포서': '마포구', '영등포서': '영등포구', '성동서': '성동구', '동작서': '동작구',
                   '광진서': '광진구', '강북서': '강북구', '금천서': '금천구', '중랑서': '중랑구',
                   '강남서': '강남구', '관악서': '관악구', '강동서': '강동구', '종암서': '성북구', 
                   '구로서': '구로구', '양천서': '양천구', '송파서': '송파구', '노원서': '노원구', 
                   '방배서': '서초구', '은평서': '은평구', '도봉서': '도봉구'}
   
   
   # dict[칼럼명].apply(칼럼 내 데이터마다 적용할 함수)
   # dict.get(key)는 value 를 return
   
   df['구별'] = df['관서명'].?(lambda x: police_to_gu.get(x, '구 없음')) # 적용하다? 
   
   # 관서별 데이터를 구별 데이터로 변경 (index : 관서 이름 -> 구 이름, column은 자동으로 오름차순 정렬됨)
   # 같은 구의 경우에는 sum 을 적용 
   
   gu_df = pd.pivot_table(df, index='구별', aggfunc=np.sum) # 피봇 테이블을 만드려면?
   
   gu_df = gu_df.drop(['구 없음']) # 행을 삭제할 때? (Database)
   ~~~~

   **범죄별로 검거율 계산**

   ~~~~python
   # 발생건수 대비 검거건수 -> 검거율 데이터 column을 범죄별로 생성
   
   gu_df['강간검거율'] = gu_df['강간(검거)']/gu_df['강간(발생)']*100
   # 검거열 을 발생열 로 나누고 * 100 하면 행 별로 계산이 됨
   
   gu_df['강도검거율'] = gu_df['강도(검거)']/gu_df['강도(발생)']*100
   gu_df['살인검거율'] = gu_df['살인(검거)']/gu_df['살인(발생)']*100
   gu_df['절도검거율'] = gu_df['절도(검거)']/gu_df['절도(발생)']*100
   gu_df['폭력검거율'] = gu_df['폭력(검거)']/gu_df['폭력(발생)']*100
   gu_df['검거율'] = gu_df['소계(검거)']/gu_df['소계(발생)']*100
   
   
   # 필요없는 column 지우기 (범죄별 발생 건수와 검거율만 남긴다)
   
   # df.drop(['row']) : 해당 행 데이터를 drop 
   # del df['column'] : 해당 열 데이터를 drop
   
   # 여러 줄을 한번에 수정할 때 커서아래로 crlt + 클릭
   del gu_df['강간(검거)']
   del gu_df['강도(검거)']
   del gu_df['살인(검거)']
   del gu_df['절도(검거)']
   del gu_df['폭력(검거)']
   del gu_df['소계(발생)']
   del gu_df['소계(검거)']
   
   // --어려운 방법 --
   # 발생건수는 2016이고, 그 전에 발생한 범죄에 대한 검거가 2016에 이뤄지면 검거수에 반영된 것
   
   columns = ['강간검거율', '강도검거율', '살인검거율', '절도검거율', '폭력검거율']
   gu_df_rate = gu_df[columns]
   
   for row_index('강남구') , row in gu_df_rate.iterrows():
       
       for column('강간검거율') in columns:
           if row[column('강간검거율')] > 100:
               gu_df.at[row_index('강남구'), column('강간검거율')] = 100 
               
   gu_df.head(10)
   iterrows() =>행을 전체 돌리는 것
   //
   
   # 위의 문제를 pandas를 통해서 풀기
   gu_df[ gu_df[['강간검거율', '강도검거율', '살인검거율', '절도검거율', '폭력검거율']] > 100] = 100
   # => 100다 큰것을 솎아 내는 '체'를 하나 만든 것
   
   
   # 새롭게 이름을 지어줄 때? rename
   
   gu_df.rename(columns = {'강간(발생)':'강간',
                           '강도(발생)':'강도',
                           '살인(발생)':'살인',
                           '절도(발생)':'절도',
                           '폭력(발생)':'폭력'}, inplace=True) # inplace 옵션 == 덮어쓰기 여부
   
   gu_df.head()
   ~~~~

   **인구 데이터 merge 하기**

   ~~~~python
   # 쉬운 순서 1. 때려붙이는게 2.인덱스로 3...?
   
   popul_df = pd.read_csv('pop_kor.csv', encoding='UTF-8') # CSV 는 어떻게 읽을까요? (엑셀과 유사)read_csv
   
   #우리는 2.인덱스 기준으로 붙이기
   # 구별 index 를 기준으로 merge를 할 것이므로, index 를 세팅해주기
   
   popul_df = pd.read_csv('pop_kor.csv', encoding='UTF-8', index_col='구별')
   popul_df.head()
   
   # 아래와 같이 먼저 read_csv()하고 .set_index()를 적용할 수도 있습니다.
   # popul_df = pd.read_csv('pop_kor.csv', encoding='UTF-8').set_index('구별')
   
   # 데이터프레임의 M&A 일반화: a.join(b)
   우리가 사용하는 것
   gu_df = gu_df.join(popul_df) # df1.join(df2) : df1 의 index를 기준으로 df2 의 index 중 매칭되는 값을 매김
   
   # df.join() 대신 pd.merge()를 사용하여 Merge할 수 있습니다. 
   # pd.merge() 자세한 설명 : https://goo.gl/3vS17P, https://goo.gl/7yr3A7
   
   # gu_df = pd.merge(gu_df, popul_df, left_index=True, right_index=True)
   
   # gu_df(left index)=>구별 pop_df(right index)=>구이름 서로 이름이 다르고 열에 들어가이 있으면 아래와 같이
   # gu_df = pd.merge(gu_df, popul_df, left_on='구별', right_on='구이름') 로 바꾸어 사용할 수 있음
   
   # gu_df = pd.merge(gu_df, popul_df, how='inner' left_on='구별', right_on='구이름' ) how='inner'/ left outer/ right outer / outer 로 줄 수 있음
   ~~~~

   

2. **데이터 탐색(Data exploration)**
   링크 들어가 보기

   - https://www.data-to-viz.com/
   - https://datavizproject.com/

   ~~~~python
   - 검거율 기준으로 데이터 정렬
   - 범죄별 발생 건수 정규화
   - 한글 데이터 시각화를 위한 준비
   - 구별 살인/절도 발생 건수 순위 살펴보기
   - 인구수로 나눠서 인구대비 발생비율로 살펴보기
   - [인구 수 대비] 구별 살인 발생 순위 살펴보기
   - [인구 수 대비] 구별 5대 범죄 발생 수치 평균 계산하기
   - [인구 수 대비 5대 범죄 발생 수치 평균] 구별 순위 비교하기
   ~~~~

   **데이터 살펴보기**

   ~~~~python
   # 검거율 기준으로 오름차순 정렬하기
   
   # '값'을 기준으로 정렬할 때? => by='검거율'
   gu_df.sort_values(by='검거율', ascending=False, inplace=True) # ascending=False : 내림차순, inplace=True : 덮어쓰기
   
   import seaborn as sns
   
   sns.heatmap(gu_df[['강간', '강도', '살인', '절도', '폭력']])
   ~~~~

   **범죄별 발생 건수 정규화 하기 (범죄별로 가장 많이 발생한 구간 1==100%)**

   ~~~~
   ML,DL 에서 많이 쓰게될 개념
   Column = Attribute = Feature (열)
   Feature의 값들이 엉망진창이다 => Feature의 scale이 엉망이면 Feature Scaling(normalization)을 통해 비슷하게 만들어줌
   1. Min-Max algtorithm
   	new X = (oldX - minX)/(MaxX - MinX)
   	사람들이 이해하기 쉬운 것은 min-max 이다.
   2. standardization
   	new X = (oldX - meanX)/(stdX) mean:평균,std:표준편차
   	왠지 어렵게 느껴짐 => 그렇지만 ML, DL에서 많이 사용함
   ~~~~

   **Feature Scaling**

   ~~~~python
   Feature Scaling
   
   # Min-Max 사용 / 아직 인구 미적용
   
   # 5대 범죄별 수치를 해당 범죄별 최대값으로 나눠줌 
   
   
   
   #weight_col 은 열마다의 최대값을 찾아줌
   weight_col = gu_df[['강간', '강도', '살인', '절도', '폭력']].max() 
   
   #각 열을 최대값으로 나누어 준다
   crime_count_norm = gu_df[['강간', '강도', '살인', '절도', '폭력']] / weight_col
   crime_count_norm
   ~~~~

   

   **한글 데이터 시각화를 위한 준비**

   ~~~~python
   # jupyter notebook 내에 figure를 보여주기
   %matplotlib inline 
   
   # matplotlib의 한글문제를 해결
   font_name = font_manager.FontProperties(fname="c:/Windows/Fonts/malgun.ttf").get_name()
   # font_name
   rc('font', family=font_name)
   
   ## Mac OS
   # rc('font', family="AppleGothic")
   ~~~~

   **구별 살인 발생 순위 살펴보기**

   ~~~~python
   sns.heatmap(crime_count_norm.sort_values(by='살인', ascending=False)) # 내림차순으로 정렬하려면?
   # => 이 그래프에서는 색깔도 반대이고, 세로축이 맞지 않는다
   
   # 몇 가지 옵션으로 더 내용을 확인하기 편하도록 수정하기
   
   # 전체 figure 의 사이즈를 조정, figure 은 도화지를 편다
   plt.figure(figsize = (10, 10))
   
   # annot : 셀 내에 수치 입력 여부
   # fmt : 셀 내 입력될 수치의 format (f == float)
   # linewidths : 셀 간 이격거리 (하얀 부분, 내부 테두리)
   # cmap : matplotlib colormap @ https://goo.gl/YWpBES
   
   
   sns.heatmap(crime_count_norm.sort_values(by='살인', ascending=False), annot=True, fmt='f', linewidths=.5, cmap='Reds') # linewidths =.5 => 0 생략 가능
   #cmap => 색깔 , 범죄는 가치 판단이 가능 하기에 나쁜것은 진한 색으로 표현 가능, annot=>픽셀안에 숫자 입력, fmt='f'float뜻함
   
   
   
   plt.title('범죄 발생(살인발생으로 정렬) - 각 항목별 최대값으로 나눠 정규화')
   plt.show()
   
   # 살인 대신 절도 기준으로 살펴보기
   
   plt.figure(figsize = (10,10))
   sns.heatmap(crime_count_norm.sort_values(by='절도', ascending=False), annot=True, fmt='f', linewidths=.5, cmap='RdPu')
   
   plt.title('범죄 발생(절도발생으로 정렬) - 각 항목별 최대값으로 나눠 정규화')
   plt.show()
   ~~~~

   **인구수로 나눠서 인구대비 발생비율로 살펴보기**

   ~~~~~python
   # 행(구)별로 구별 범죄 수 (max 대비 비율값) / 구별 인구 수 * 100000 
   # 인구 수 단위인 10만을 곱해준다 (강서구 강간 = 9.795665e-07 -> 0.x 까지 끌어올리기)
   
   # 나눌 대상.div(나누는 수, 인덱스열이 맞춰 주어야 함)
   
   crime_ratio = crime_count_norm.div(gu_df['인구수'], axis='index') * 100000 
   crime_ratio.head()
   
   => 인구수에 따른 발생비율 (1)
   ~~~~~

   **[인구수 대비] 구별 살인 발생 순위 살펴보기**

   ~~~~python
   plt.figure(figsize = (10,10))
   
   sns.heatmap(crime_ratio.sort_values(by='살인', ascending=False), annot=True, fmt='f', linewidths=.5, cmap='Reds')
   plt.title('범죄 발생(살인발생으로 정렬) - 각 항목을 정규화한 후 인구로 나눔')
   plt.show()
   
   =>(1) 을 기준으로 sort
   ~~~~

   **[인구수 대비] 구별 5대범죄 발생 수치 평균**

   ~~~~python
   crime_ratio.mean() => 이것은 그냥 각각의 평균
   
   # 구별 인구 대비 
   # axis=0 => 열 방향 , axis= 행 방향
   crime_ratio['전체발생비율'] = crime_ratio.mean(axis=1) # 가중치를 줘야 하는데 이것은 이미 가중치가 되어있음
   crime_ratio.head()
   
   => (1)의 5대 범죄의 합의 평균을 구별로 구한 것
   ~~~~

   **[ 인구수 대비 ] [5대 범죄 발생 수치 평균] 기준 구별 순위 비교**

   ~~~~~python
   plt.figure(figsize = (10,10))
   
   sns.heatmap(crime_ratio.sort_values(by='절도', ascending=False), annot=True, fmt='f', linewidths=.5, cmap='Reds')
   plt.title('범죄 발생(살인발생으로 정렬) - 각 항목을 정규화한 후 인구로 나눔')
   plt.show()
   ~~~~~

   

3. **데이터 시각화**

   ~~~~
   지도 시각화 :Folium library 활용
   지도 데이터 :GeoJSON => 국가에 맞게 써라
   ~~~~

   ~~~~python
   !pip install folium==0.5.0
   ~~~~

   ~~~~python
   # import warnings
   # warnings.simplefilter(action = "ignore", category = FutureWarning)
   
   import json
   
   geo_path = 'skorea_municipalities_geo_simple.json'
   geo_str = json.load(open(geo_path, encoding='utf-8'))
   
   #json.dump(~~~) json type으로 바꿔줌
   ~~~~

   **JSON 구조를 쉽게 파악할 수 있게 해주는 도구들 : 1) pyprnt**

   ~~~~python
   # PyPrnt @ http://j.mp/2WVZuGy
   
   !pip install pyprnt==2.3.3
   
   from pyprnt import prnt
   
   
   menu = {
       "Kimchi": 5000,
       "Ice Cream": 100
   }
   prnt(menu)
   
   .. 예제 참조
   ~~~~

   **구별 살인사건 발생 건수 시각화**

   ~~~~python
   import folium
   
   # tiles : 지도 타입 (default type or "Stamen Terrain" or "Stamen Toner")
   # location : 초기 지도 center 위치
   map = folium.Map(location=[37.5502, 126.982], zoom_start=11, tiles='Stamen Toner') 
   
   
   map = folium.Map(location=[37.5502, 126.982], zoom_start=11, tiles='Stamen Toner')
   
   # 살인사건 발생건수 시각화
   # Choropleth map : 정의 @ https://goo.gl/yrTRHU, folium 공식문서 @ https://goo.gl/5UgneX
   # Another available library for Choropleth map : Altair @ https://altair-viz.github.io/gallery/choropleth.html
   
   #choropleth => 지도기반으로 수치를 가지고 행정구역별 색을 칠함
   map.choropleth(geo_data = geo_str, # 서울시 행정구역별 polygon drawing
                  
   data = gu_df['살인'], # 시각화의 대상이 될 데이터
   columns = [gu_df.index, gu_df['살인']], # 1) df의 index 칼럼을 가져와 인식하고 ,[행정구역, 동일한 df]
                 
                  
   fill_color = 'PuRd', #PuRd, YlGnBu <- color brewer (http://colorbrewer2.org/) : ‘BuGn’, ‘BuPu’, ‘GnBu’, ‘OrRd’, ‘PuBu’, ‘PuBuGn’, ‘PuRd’, ‘RdPu’, ‘YlGn’, ‘YlGnBu’, ‘YlOrBr’, and ‘YlOrRd’
   
   key_on = 'feature.id') # GeoJSON 규약을 따름, json 파일(지도 데이터)의 "feature" type의 "id" 에 매칭된다
   
   # key_on: Variable in the GeoJSON file to bind the data to. 
   # Must always start with 'feature' and be in JavaScript objection notation. 
   # Ex: 'feature.id' or 'feature.properties.statename'.
   
   map
   ~~~~

   검거율 구별로 시각화 해놓음 =>같은 구지만 못하는 곳이 있으면 낮게 나옴

